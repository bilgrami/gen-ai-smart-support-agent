{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEe53ei2lG5iv5UJX6N046",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bilgrami/gen-ai-smart-support-agent/blob/feature%2F00-kb-articles/Data_Ingestion_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Ingestion API Implementation\n",
        "\n",
        "This notebook sets up a simple data ingestion system using Delta Lake and PySpark. It allows you to register datasets, start ingestion jobs, and check their status.\n",
        "\n",
        "## Step 1: Install Required Packages\n",
        "\n",
        "We need to install some Python packages to make everything work. Run this cell to install them:\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "TVZhALah6EHB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN77EhDg51uR",
        "outputId": "a7499fea-ed44-416e-ff07-85fb78285bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.5.1\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting delta-spark==3.2.0\n",
            "  Downloading delta_spark-3.2.0-py3-none-any.whl (21 kB)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.34.140-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Collecting azure-storage-blob\n",
            "  Downloading azure_storage_blob-12.20.0-py3-none-any.whl (392 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m392.2/392.2 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting oci\n",
            "  Downloading oci-2.129.1-py3-none-any.whl (26.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.8/26.8 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.5.1) (0.10.9.7)\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark==3.2.0) (8.0.0)\n",
            "Collecting botocore<1.35.0,>=1.34.140 (from boto3)\n",
            "  Downloading botocore-1.34.140-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.27.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.16.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.7.1)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.31.0)\n",
            "Collecting azure-core>=1.28.0 (from azure-storage-blob)\n",
            "  Downloading azure_core-1.30.2-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.3/194.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (42.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (4.12.2)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from oci) (2024.6.2)\n",
            "Requirement already satisfied: pyOpenSSL<25.0.0,>=17.5.0 in /usr/local/lib/python3.10/dist-packages (from oci) (24.1.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from oci) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2016.10 in /usr/local/lib/python3.10/dist-packages (from oci) (2023.4)\n",
            "Collecting circuitbreaker<2.0.0,>=1.3.1 (from oci)\n",
            "  Downloading circuitbreaker-1.4.0.tar.gz (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.28.0->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.140->boto3) (2.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.63.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (3.20.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.0.0->delta-spark==3.2.0) (3.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.6.0)\n",
            "Building wheels for collected packages: pyspark, circuitbreaker\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=88fae51833c927d3cf17017c6307932785418f9f2fddb0315ad5d3f328323a29\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "  Building wheel for circuitbreaker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for circuitbreaker: filename=circuitbreaker-1.4.0-py3-none-any.whl size=7520 sha256=0e8f69880dd5b5d25b5a399949628127119d0389274fcd6b3ce5bc52bd7f658e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/be/64/266b6ce2ef1130de5e419f04805acbb2df5a4ab1b91348f25b\n",
            "Successfully built pyspark circuitbreaker\n",
            "Installing collected packages: circuitbreaker, pyspark, jmespath, isodate, delta-spark, botocore, azure-core, s3transfer, azure-storage-blob, oci, boto3\n",
            "Successfully installed azure-core-1.30.2 azure-storage-blob-12.20.0 boto3-1.34.140 botocore-1.34.140 circuitbreaker-1.4.0 delta-spark-3.2.0 isodate-0.6.1 jmespath-1.0.1 oci-2.129.1 pyspark-3.5.1 s3transfer-0.10.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.5.1 delta-spark==3.2.0 boto3 google-cloud-storage azure-storage-blob oci"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import Libraries and Set Up Spark\n",
        "\n",
        "This step imports necessary Python libraries and sets up our Spark environment. Spark is a tool that helps us process large amounts of data quickly."
      ],
      "metadata": {
        "id": "60prt_iY6Kcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from delta import *\n",
        "\n",
        "builder = SparkSession.builder.appName(\"DeltaLakeQuickStart\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
      ],
      "metadata": {
        "id": "1H52NLHQ6Pf9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Set Up Storage Provider\n",
        "\n",
        "This function helps us work with different storage systems, like your local computer or cloud storage services."
      ],
      "metadata": {
        "id": "tMmSb6DN6XJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def get_file_path(path, provider='local'):\n",
        "    if provider == 'local':\n",
        "        return os.path.join('/content', path)\n",
        "    elif provider == 'aws':\n",
        "        return f\"s3a://{path}\"\n",
        "    elif provider == 'gcp':\n",
        "        return f\"gs://{path}\"\n",
        "    elif provider == 'azure':\n",
        "        return f\"wasbs://{path}\"\n",
        "    elif provider == 'oracle':\n",
        "        return f\"oci://{path}\"\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported storage provider\")"
      ],
      "metadata": {
        "id": "ZsBuM_UU6b6u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Create Functions for Working with Data Tables\n",
        "\n",
        "This function lets us read from and write to our data tables easily."
      ],
      "metadata": {
        "id": "_sjiD_JZ6grq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from delta import DeltaTable\n",
        "\n",
        "def delta_operation(path, mode='read', data=None, provider='local'):\n",
        "    full_path = get_file_path(path, provider)\n",
        "    if mode == 'read':\n",
        "        if DeltaTable.isDeltaTable(spark, full_path):\n",
        "            return spark.read.format(\"delta\").load(full_path)\n",
        "        else:\n",
        "            return spark.createDataFrame([], schema=StructType([]))\n",
        "    elif mode == 'write':\n",
        "        data.write.format(\"delta\").mode(\"overwrite\").save(full_path)\n",
        "    elif mode == 'upsert':\n",
        "        if DeltaTable.isDeltaTable(spark, full_path):\n",
        "            delta_table = DeltaTable.forPath(spark, full_path)\n",
        "            delta_table.alias(\"old\").merge(\n",
        "                data.alias(\"new\"),\n",
        "                \"old.id = new.id\"\n",
        "            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
        "        else:\n",
        "            data.write.format(\"delta\").mode(\"overwrite\").save(full_path)"
      ],
      "metadata": {
        "id": "INmxQzZV7Y53"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Create API Functions\n",
        "\n",
        "These functions form the core of our API. They allow us to register datasets, start ingestion jobs, and check job status."
      ],
      "metadata": {
        "id": "AtTCfiS37cxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "def get_ingestion_status(job_id=None, dataset_id=None):\n",
        "    table_path = get_file_path(\"ingestion_logs\")\n",
        "    if not DeltaTable.isDeltaTable(spark, table_path):\n",
        "        return []  # Return an empty list if the table doesn't exist\n",
        "\n",
        "    ingestion_logs = spark.read.format(\"delta\").load(table_path)\n",
        "\n",
        "    if job_id:\n",
        "        ingestion_logs = ingestion_logs.filter(col(\"jobId\") == job_id)\n",
        "    if dataset_id:\n",
        "        ingestion_logs = ingestion_logs.filter(col(\"datasetId\") == dataset_id)\n",
        "\n",
        "    # Check the data type of startTime and endTime\n",
        "    start_time_type = ingestion_logs.schema[\"startTime\"].dataType\n",
        "    end_time_type = ingestion_logs.schema[\"endTime\"].dataType\n",
        "\n",
        "    # If they're already strings, just select them as is\n",
        "    if isinstance(start_time_type, StringType) and isinstance(end_time_type, StringType):\n",
        "        selected_logs = ingestion_logs\n",
        "    else:\n",
        "        # If they're timestamps, convert to strings\n",
        "        selected_logs = ingestion_logs.select(\n",
        "            \"*\",\n",
        "            when(col(\"startTime\").isNotNull(), col(\"startTime\").cast(\"string\")).otherwise(None).alias(\"startTime\"),\n",
        "            when(col(\"endTime\").isNotNull(), col(\"endTime\").cast(\"string\")).otherwise(None).alias(\"endTime\")\n",
        "        )\n",
        "\n",
        "    return selected_logs.toPandas().to_dict(orient=\"records\")\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "from datetime import datetime\n",
        "\n",
        "def start_ingestion(ingestion_info):\n",
        "    # Define the schema explicitly\n",
        "    schema = StructType([\n",
        "        StructField(\"jobId\", StringType(), True),\n",
        "        StructField(\"datasetId\", StringType(), True),\n",
        "        StructField(\"status\", StringType(), True),\n",
        "        StructField(\"startTime\", StringType(), True),\n",
        "        StructField(\"endTime\", StringType(), True),\n",
        "        StructField(\"errorMessage\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # Check if the ingestion_logs Delta table exists\n",
        "    table_path = get_file_path(\"ingestion_logs\")\n",
        "    if not DeltaTable.isDeltaTable(spark, table_path):\n",
        "        # If it doesn't exist, create an empty Delta table with the correct schema\n",
        "        empty_df = spark.createDataFrame([], schema=schema)\n",
        "        empty_df.write.format(\"delta\").save(table_path)\n",
        "\n",
        "    # Read existing ingestion logs\n",
        "    ingestion_logs = spark.read.format(\"delta\").load(table_path)\n",
        "\n",
        "    # Create a new log entry\n",
        "    current_time = datetime.now().isoformat()\n",
        "    new_log = spark.createDataFrame([(\n",
        "        ingestion_info['jobId'],\n",
        "        ingestion_info['datasetId'],\n",
        "        \"running\",\n",
        "        current_time,\n",
        "        None,\n",
        "        None\n",
        "    )], schema=schema)\n",
        "\n",
        "    # Union the existing logs with the new one and remove duplicates\n",
        "    updated_logs = ingestion_logs.union(new_log).dropDuplicates([\"jobId\"])\n",
        "\n",
        "    # Write the updated logs back to the Delta table\n",
        "    updated_logs.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
        "\n",
        "    return {\"status\": \"success\", \"message\": \"Ingestion job started\", \"jobId\": ingestion_info['jobId']}\n",
        ""
      ],
      "metadata": {
        "id": "gDEHjiv17gYx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Set Up Initial Data\n",
        "\n",
        "This step creates some initial data for our system, including API documentation and knowledge base articles."
      ],
      "metadata": {
        "id": "45Lltxi57lTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# API Documentation\n",
        "api_doc = {\n",
        "  \"openapi\": \"3.0.0\",\n",
        "  \"info\": {\n",
        "    \"title\": \"Data Ingestion API\",\n",
        "    \"version\": \"1.0.0\",\n",
        "    \"description\": \"API for registering datasets and managing data ingestion into the silver area of the data lake.\"\n",
        "  },\n",
        "  \"paths\": {\n",
        "    \"/api/table\": {\n",
        "      \"post\": {\n",
        "        \"summary\": \"Register a new dataset\",\n",
        "        \"requestBody\": {\n",
        "          \"required\": True,\n",
        "          \"content\": {\n",
        "            \"application/json\": {\n",
        "              \"schema\": {\n",
        "                \"$ref\": \"#/components/schemas/DatasetRegistration\"\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        },\n",
        "        \"responses\": {\n",
        "          \"200\": {\n",
        "            \"description\": \"Successful registration\",\n",
        "            \"content\": {\n",
        "              \"application/json\": {\n",
        "                \"schema\": {\n",
        "                  \"$ref\": \"#/components/schemas/RegistrationResponse\"\n",
        "                }\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      },\n",
        "      \"get\": {\n",
        "        \"summary\": \"Retrieve registered datasets\",\n",
        "        \"parameters\": [\n",
        "          {\n",
        "            \"name\": \"datasetName\",\n",
        "            \"in\": \"query\",\n",
        "            \"schema\": {\n",
        "              \"type\": \"string\"\n",
        "            }\n",
        "          }\n",
        "        ],\n",
        "        \"responses\": {\n",
        "          \"200\": {\n",
        "            \"description\": \"List of registered datasets\",\n",
        "            \"content\": {\n",
        "              \"application/json\": {\n",
        "                \"schema\": {\n",
        "                  \"$ref\": \"#/components/schemas/DatasetList\"\n",
        "                }\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    \"/api/ingestion\": {\n",
        "      \"post\": {\n",
        "        \"summary\": \"Trigger ingestion process\",\n",
        "        \"requestBody\": {\n",
        "          \"required\": True,\n",
        "          \"content\": {\n",
        "            \"application/json\": {\n",
        "              \"schema\": {\n",
        "                \"$ref\": \"#/components/schemas/IngestionRequest\"\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        },\n",
        "        \"responses\": {\n",
        "          \"200\": {\n",
        "            \"description\": \"Ingestion job started\",\n",
        "            \"content\": {\n",
        "              \"application/json\": {\n",
        "                \"schema\": {\n",
        "                  \"$ref\": \"#/components/schemas/IngestionResponse\"\n",
        "                }\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      },\n",
        "      \"get\": {\n",
        "        \"summary\": \"Retrieve ingestion job status\",\n",
        "        \"parameters\": [\n",
        "          {\n",
        "            \"name\": \"jobId\",\n",
        "            \"in\": \"query\",\n",
        "            \"schema\": {\n",
        "              \"type\": \"string\"\n",
        "            }\n",
        "          },\n",
        "          {\n",
        "            \"name\": \"datasetId\",\n",
        "            \"in\": \"query\",\n",
        "            \"schema\": {\n",
        "              \"type\": \"string\"\n",
        "            }\n",
        "          }\n",
        "        ],\n",
        "        \"responses\": {\n",
        "          \"200\": {\n",
        "            \"description\": \"List of ingestion jobs\",\n",
        "            \"content\": {\n",
        "              \"application/json\": {\n",
        "                \"schema\": {\n",
        "                  \"$ref\": \"#/components/schemas/JobStatusList\"\n",
        "                }\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"components\": {\n",
        "    \"schemas\": {\n",
        "      \"DatasetRegistration\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"datasetName\": {\"type\": \"string\"},\n",
        "          \"sourceFileOptions\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "              \"delimiter\": {\"type\": \"string\"},\n",
        "              \"hasHeader\": {\"type\": \"boolean\"},\n",
        "              \"fileType\": {\"type\": \"string\", \"enum\": [\"csv\", \"json\", \"parquet\", \"xls\", \"orc\"]}\n",
        "            }\n",
        "          },\n",
        "          \"sourceColumns\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "              \"type\": \"object\",\n",
        "              \"properties\": {\n",
        "                \"name\": {\"type\": \"string\"},\n",
        "                \"ordinalNumber\": {\"type\": \"integer\"},\n",
        "                \"dataType\": {\"type\": \"string\"},\n",
        "                \"sampleData\": {\"type\": \"string\"}\n",
        "              }\n",
        "            }\n",
        "          },\n",
        "          \"targetColumns\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "              \"type\": \"object\",\n",
        "              \"properties\": {\n",
        "                \"name\": {\"type\": \"string\"},\n",
        "                \"ordinalNumber\": {\"type\": \"integer\"},\n",
        "                \"dataType\": {\"type\": \"string\"},\n",
        "                \"sampleData\": {\"type\": \"string\"},\n",
        "                \"sourceColumnName\": {\"type\": \"string\"},\n",
        "                \"transformationFunction\": {\"type\": \"string\"}\n",
        "              }\n",
        "            }\n",
        "          },\n",
        "          \"computeRowChecksum\": {\"type\": \"boolean\"},\n",
        "          \"computeColumnChecksum\": {\"type\": \"boolean\"},\n",
        "          \"computeLineage\": {\"type\": \"boolean\"}\n",
        "        }\n",
        "      },\n",
        "      \"RegistrationResponse\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"status\": {\"type\": \"string\"},\n",
        "          \"message\": {\"type\": \"string\"},\n",
        "          \"datasetId\": {\"type\": \"string\"}\n",
        "        }\n",
        "      },\n",
        "      \"DatasetList\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"datasets\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "              \"$ref\": \"#/components/schemas/DatasetRegistration\"\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      },\n",
        "      \"IngestionRequest\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"datasetId\": {\"type\": \"string\"},\n",
        "          \"sourceFilePath\": {\"type\": \"string\"}\n",
        "        }\n",
        "      },\n",
        "      \"IngestionResponse\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"status\": {\"type\": \"string\"},\n",
        "          \"message\": {\"type\": \"string\"},\n",
        "          \"jobId\": {\"type\": \"string\"}\n",
        "        }\n",
        "      },\n",
        "      \"JobStatusList\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"jobs\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "              \"type\": \"object\",\n",
        "              \"properties\": {\n",
        "                \"jobId\": {\"type\": \"string\"},\n",
        "                \"datasetId\": {\"type\": \"string\"},\n",
        "                \"status\": {\"type\": \"string\", \"enum\": [\"running\", \"completed\", \"failed\"]},\n",
        "                \"startTime\": {\"type\": \"string\"},\n",
        "                \"endTime\": {\"type\": \"string\"},\n",
        "                \"errorMessage\": {\"type\": \"string\"}\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "api_doc_df = spark.createDataFrame([(json.dumps(api_doc),)], [\"doc\"])\n",
        "delta_operation(\"api_documentation\", \"write\", api_doc_df)\n",
        "\n",
        "# KB Articles\n",
        "kb_articles = {\n",
        "    \"articles\": [\n",
        "      {\n",
        "        \"kbArticleId\": \"TS001\",\n",
        "        \"AppName\": \"pipeline\",\n",
        "        \"AppVersion\": \"1\",\n",
        "        \"ArticleTitle\": \"Table Setup Guide\",\n",
        "        \"Content\": \"This guide provides information on setting up tables for data ingestion using our Data Ingestion API.\",\n",
        "        \"StepByStepInstructions\": \"1. Prepare Your Dataset\\n2. Register the Dataset\\n3. Verify Registration\\n4. Configure Checksums and Lineage\",\n",
        "        \"OwnerTeamName\": \"ingestion-team\",\n",
        "        \"TeamContact\": \"Jane Doe\",\n",
        "        \"WrittenBy\": \"John Smith\",\n",
        "        \"ArticleVersion\": \"1.0\",\n",
        "        \"CreatedOn\": \"2024-07-01\",\n",
        "        \"CreatedBy\": \"John Smith\",\n",
        "        \"ModifiedOn\": \"2024-07-01\",\n",
        "        \"ModifiedBy\": \"John Smith\",\n",
        "        \"IssueCategory\": \"table-setup\"\n",
        "      },\n",
        "      {\n",
        "        \"kbArticleId\": \"IN001\",\n",
        "        \"AppName\": \"pipeline\",\n",
        "        \"AppVersion\": \"1\",\n",
        "        \"ArticleTitle\": \"Data Ingestion Guide\",\n",
        "        \"Content\": \"This guide covers the process of ingesting data into the silver area of the data lake using our Data Ingestion API.\",\n",
        "        \"StepByStepInstructions\": \"1. Prepare for Ingestion\\n2. Trigger Ingestion\\n3. Monitor Ingestion Progress\\n4. Verify Ingested Data\",\n",
        "        \"OwnerTeamName\": \"ingestion-team\",\n",
        "        \"TeamContact\": \"Jane Doe\",\n",
        "        \"WrittenBy\": \"Alice Johnson\",\n",
        "        \"ArticleVersion\": \"1.0\",\n",
        "        \"CreatedOn\": \"2024-07-02\",\n",
        "        \"CreatedBy\": \"Alice Johnson\",\n",
        "        \"ModifiedOn\": \"2024-07-02\",\n",
        "        \"ModifiedBy\": \"Alice Johnson\",\n",
        "        \"IssueCategory\": \"ingestion\"\n",
        "      },\n",
        "      {\n",
        "        \"kbArticleId\": \"JS001\",\n",
        "        \"AppName\": \"job-runner\",\n",
        "        \"AppVersion\": \"2\",\n",
        "        \"ArticleTitle\": \"Job Submission Guide\",\n",
        "        \"Content\": \"This guide explains how to submit and manage data ingestion jobs using our Data Ingestion API.\",\n",
        "        \"StepByStepInstructions\": \"1. Prepare Job Parameters\\n2. Submit the Job\\n3. Track Job Progress\\n4. Handle Job Completion\",\n",
        "        \"OwnerTeamName\": \"consumption-team\",\n",
        "        \"TeamContact\": \"Bob Wilson\",\n",
        "        \"WrittenBy\": \"Emma Brown\",\n",
        "        \"ArticleVersion\": \"1.0\",\n",
        "        \"CreatedOn\": \"2024-07-03\",\n",
        "        \"CreatedBy\": \"Emma Brown\",\n",
        "        \"ModifiedOn\": \"2024-07-03\",\n",
        "        \"ModifiedBy\": \"Emma Brown\",\n",
        "        \"IssueCategory\": \"job-submission\"\n",
        "      },\n",
        "      {\n",
        "        \"kbArticleId\": \"CF001\",\n",
        "        \"AppName\": \"query-engine\",\n",
        "        \"AppVersion\": \"3\",\n",
        "        \"ArticleTitle\": \"Configuration Guide\",\n",
        "        \"Content\": \"This guide covers the configuration options available for dataset registration and ingestion jobs in our Data Ingestion API.\",\n",
        "        \"StepByStepInstructions\": \"1. Configure Dataset Options\\n2. Set Up Column Configurations\\n3. Enable Data Quality Options\\n4. Configure Ingestion Job Parameters\",\n",
        "        \"OwnerTeamName\": \"support\",\n",
        "        \"TeamContact\": \"Charlie Green\",\n",
        "        \"WrittenBy\": \"David Lee\",\n",
        "        \"ArticleVersion\": \"1.0\",\n",
        "        \"CreatedOn\": \"2024-07-04\",\n",
        "        \"CreatedBy\": \"David Lee\",\n",
        "        \"ModifiedOn\": \"2024-07-04\",\n",
        "        \"ModifiedBy\": \"David Lee\",\n",
        "        \"IssueCategory\": \"configuration\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "\n",
        "kb_articles_df = spark.createDataFrame(kb_articles['articles'])\n",
        "delta_operation(\"kb_articles\", \"write\", kb_articles_df)\n"
      ],
      "metadata": {
        "id": "7SQBCpoH7okx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Use the API\n",
        "\n",
        "Now that everything is set up, we can use our API functions. This example shows how to register a dataset, start an ingestion job, and check its status."
      ],
      "metadata": {
        "id": "3OUL8ikA8MIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register a dataset\n",
        "dataset_info1 = {\n",
        "    \"datasetId\": \"DS001\",\n",
        "    \"datasetName\": \"SampleDataset\",\n",
        "    \"sourceFileOptions\": {\n",
        "        \"delimiter\": \",\",\n",
        "        \"hasHeader\": True,\n",
        "        \"fileType\": \"csv\"\n",
        "    },\n",
        "    \"sourceColumns\": [\n",
        "        {\"name\": \"id\", \"ordinalNumber\": \"1\", \"dataType\": \"int\", \"sampleData\": \"1\"},\n",
        "        {\"name\": \"name\", \"ordinalNumber\": \"2\", \"dataType\": \"string\", \"sampleData\": \"John Doe\"}\n",
        "    ],\n",
        "    \"targetColumns\": [\n",
        "        {\"name\": \"id\", \"ordinalNumber\": \"1\", \"dataType\": \"int\", \"sampleData\": \"1\", \"sourceColumnName\": \"id\"},\n",
        "        {\"name\": \"full_name\", \"ordinalNumber\": \"2\", \"dataType\": \"string\", \"sampleData\": \"John Doe\", \"sourceColumnName\": \"name\"}\n",
        "    ],\n",
        "    \"computeRowChecksum\": True,\n",
        "    \"computeColumnChecksum\": False,\n",
        "    \"computeLineage\": True\n",
        "}\n",
        "\n",
        "dataset_info = {\n",
        "    \"datasetId\": \"DS001\",\n",
        "    \"datasetName\": \"SampleDataset\",\n",
        "    \"sourceFileOptions\": {\n",
        "        \"delimiter\": \",\",\n",
        "        \"hasHeader\": \"true\",\n",
        "        \"fileType\": \"csv\"\n",
        "    },\n",
        "    \"sourceColumns\": [\n",
        "        {\"name\": \"id\", \"ordinalNumber\": \"1\", \"dataType\": \"int\", \"sampleData\": \"1\"},\n",
        "        {\"name\": \"name\", \"ordinalNumber\": \"2\", \"dataType\": \"string\", \"sampleData\": \"John Doe\"}\n",
        "    ],\n",
        "    \"targetColumns\": [\n",
        "        {\"name\": \"id\", \"ordinalNumber\": \"1\", \"dataType\": \"int\", \"sampleData\": \"1\", \"sourceColumnName\": \"id\"},\n",
        "        {\"name\": \"full_name\", \"ordinalNumber\": \"2\", \"dataType\": \"string\", \"sampleData\": \"John Doe\", \"sourceColumnName\": \"name\"}\n",
        "    ],\n",
        "    \"computeRowChecksum\": True,\n",
        "    \"computeColumnChecksum\": False,\n",
        "    \"computeLineage\": True\n",
        "}\n",
        "result = register_dataset(dataset_info)\n",
        "print(\"Register dataset result:\", result)\n",
        "\n",
        "\n",
        "\n",
        "# Start ingestion\n",
        "ingestion_info = {\n",
        "    \"jobId\": \"JOB001\",\n",
        "    \"datasetId\": \"DS001\",\n",
        "    \"sourceFilePath\": \"/path/to/source/file.csv\"\n",
        "}\n",
        "\n",
        "result = start_ingestion(ingestion_info)\n",
        "print(\"Start ingestion result:\", result)\n",
        "\n",
        "# Get ingestion status\n",
        "status = get_ingestion_status(job_id=\"JOB001\")\n",
        "print(\"Ingestion status:\", status)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WliUL0hG8NGA",
        "outputId": "480615ee-bfb4-40f6-b31e-859c453b331d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Register dataset result: {'status': 'success', 'message': 'Dataset registered successfully', 'datasetId': 'DS001'}\n",
            "Start ingestion result: {'status': 'success', 'message': 'Ingestion job started', 'jobId': 'JOB001'}\n",
            "Ingestion status: [{'jobId': 'JOB001', 'datasetId': 'DS001', 'status': 'running', 'startTime': '2024-07-06 06:18:33.96634', 'endTime': None, 'errorMessage': None}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Clean Up\n",
        "\n",
        "When you're done, it's good practice to close the Spark session to free up resources."
      ],
      "metadata": {
        "id": "Rj-haM6Y80Kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "H79mDEJN86Xp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}